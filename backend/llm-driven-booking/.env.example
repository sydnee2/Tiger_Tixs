# LLM-driven-booking service environment variables
# Copy this file to `.env` for local development.
# Do NOT commit real secrets; use Render dashboard for production.

# Server
PORT=6101
ALLOWED_ORIGIN=http://localhost:3000,https://tiger-tixs.vercel.app/
CLIENT_BASE=https://client-service-try9.onrender.com

# LLM provider selection: 'openai' or 'ollama'
LLM_PROVIDER=openai

# OpenAI settings
# Set your key locally only; in Render, set OPENAI_API_KEY via dashboard Secret
OPENAI_API_KEY=sk-your-local-dev-key
OPENAI_MODEL=gpt-4o-mini

# Optional: override base URL for OpenAI-compatible providers (e.g., OpenRouter)
# OPENAI_BASE_URL=https://openrouter.ai/api/v1/chat/completions

# Ollama settings (for local dev)
# OLLAMA_HOST=http://localhost:11434
# OLLAMA_MODEL=llama3.1:latest
